{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Extraction et Prédiction à partir d'une Vidéo\n",
        "\n",
        "Ce notebook contient le code permettant d'obtenir des prédictions des lettres de l'alphabet du langage des signes américain (ASL) à partir d'une vidéo.\n",
        "\n",
        "### Méthode d'extraction des images\n",
        "Pour réaliser les prédictions, nous avons choisi d'extraire **2 images par seconde (fps)** à partir de la vidéo.\n",
        "\n",
        "### Stratégie pour améliorer les prédictions\n",
        "Les modèles générés par notre réseau de neurones, n'étant pas très fiable fiable et de précis. En particulier, une même image peut parfois être associée à des lettres différentes selon les prédictions du modèle. Nous avons décidé pour y remédier de:\n",
        "1. Passer **chaque image 5 fois** dans le même modèle.\n",
        "2. Attribuer comme prédiction finale la lettre ayant la **plus grande occurrence parmi les trois résultats**.\n",
        "\n",
        "### Gestion des répétitions dans les prédictions\n",
        "Un autre problème est apparu lors du traitement : en extrayant 2 images par seconde, il arrive que plusieurs images consécutives soient associées à la même lettre, même lorsque ce n'est pas le cas dans la vidéo originale. Par exemple, si une personne énonce \"Emma\", le résultat brut pourrait inclure des lettres répétées comme \"Eemmaaa\". Puisque nous ne pouvons pas prédire à quelle vitesse la personne va signer dans la vidéo.\n",
        "\n",
        "Pour éviter ces répétitions et améliorer la lisibilité, nous avons choisi de conserver uniquement les lettres **différentes** dans la séquence finale. Ainsi :\n",
        "- Si une lettre est détectée plusieurs fois de suite, elle est comptée une seule fois.\n",
        "- Par exemple, pour le prénom \"Emma\", le résultat serait \"Ema\".\n",
        "\n",
        "Nous avons fait ce choix en nous basant sur le fait que **la compréhension du message est prioritaire par rapport à l'orthographe exacte**.\n",
        "\n"
      ],
      "metadata": {
        "id": "_MJJMP-mn_js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports de librairies"
      ],
      "metadata": {
        "id": "0AjR0Nb1rvqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "id": "yBvvlVprusE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fFSb1keonkBE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import shutil\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import du modèle"
      ],
      "metadata": {
        "id": "Y9e53foeXByA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'gpu')\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "I4oCVs-IXwNz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'vide']\n",
        "num_classes = len(class_names) # = 27\n",
        "label_to_class = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z', 26: 'vide'}"
      ],
      "metadata": {
        "id": "fp-BwVZxMxrB"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les 2 codes suivant est a adapté suivant le modèle que l'on télécharge"
      ],
      "metadata": {
        "id": "O1U0-LMBbHBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "cMPV8EJ0LGZt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.convolution = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5), #output_shape=(batch_size, 16, 196, 196)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2), #output_shape=(batch_size, 16, 98, 98)\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5), #output_shape=(batch_size, 32, 94, 94)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2), #output_shape=(batch_size, 32, 47, 47)\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5), #output_shape=(batch_size, 64, 43, 43)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5), #output_shape=(batch_size, 128, 39, 39)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2), #output_shape=(batch_size, 128, 19, 19)\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 19 * 19, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.convolution(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "w6MdUdq0bC9F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chargement modèle au format .pth"
      ],
      "metadata": {
        "id": "1uqmiEWabM_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'full_model.pth'\n",
        "\n",
        "checkpoint = torch.load(filename)\n",
        "model = ImageClassifier(num_classes=num_classes).to(device)  # Dépend du modèle utilisé\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer = torch.optim.SGD(model.parameters())\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']"
      ],
      "metadata": {
        "id": "n1Vefg1cXEKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Découpage de la vidéo"
      ],
      "metadata": {
        "id": "C5IHl5rtr_4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deleteDir(path):\n",
        "  \"\"\"\n",
        "  Cette fonction permet de supprimer un dossier et son contenu.\n",
        "\n",
        "  Arguments:\n",
        "  - path: chemin du dossier à supprimer\n",
        "  \"\"\"\n",
        "  if os.path.exists(path):\n",
        "    shutil.rmtree(path)"
      ],
      "metadata": {
        "id": "K6SVFtrptJz8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoupeVideo(video_path, output_folder):\n",
        "  \"\"\"\n",
        "  Cette fonction permet de découper une vidéo en images (2 images par secondes).\n",
        "  Elle découpe la vidéo en images et les enregistres dans le dossier 'output_folder'.\n",
        "\n",
        "  Arguments:\n",
        "  - video_path: chemin vers la vidéo à découper\n",
        "  - output_folder: dossier où les images seront enregistrées.\n",
        "  \"\"\"\n",
        "\n",
        "  # Si le dossier d'extraction existe déjà --> suppression\n",
        "  # Pour supprimer des images potentiellement\n",
        "  # déjà découpées d'une vidéo précédente\n",
        "  deleteDir(output_folder)\n",
        "  os.makedirs(output_folder)  # création du dossier d'extraction\n",
        "\n",
        "  video = cv2.VideoCapture(video_path) # Ouverture vidéo\n",
        "\n",
        "  # Vérification de l'ouverture de la vidéo\n",
        "  if not video.isOpened():\n",
        "      print(\"Erreur ouverture vidéo (verifiez si le chemin de la video est ok)\")\n",
        "  else:\n",
        "\n",
        "      fps = video.get(cv2.CAP_PROP_FPS)  # Frames Par Seconde\n",
        "\n",
        "      frame_cpt = 0  # Compteur pour nommer les images extraites\n",
        "      FRAME_INTERVAL = int(fps // 2) # On extrait 2 images par secondes\n",
        "\n",
        "      while True:  # Tant qu'on a pas lu toute la vidéo\n",
        "          # Lecture frame 1\n",
        "          success, frame = video.read()\n",
        "          if not success:\n",
        "              break  # Fin de la vidéo\n",
        "\n",
        "          # Si frame lue, on saute les frames nécessaires pour obtenir 2 fps\n",
        "          for i in range(FRAME_INTERVAL - 1):\n",
        "              success, _ = video.read()\n",
        "              if not success:\n",
        "                  break  # Fin de la vidéo\n",
        "\n",
        "          # Sauvegarde de l'image\n",
        "          if success:\n",
        "              frame_filename = os.path.join(output_folder, f\"{os.path.basename(video_path)}_{frame_cpt:04d}.jpg\")\n",
        "              cv2.imwrite(frame_filename, frame)\n",
        "              frame_cpt += 1\n",
        "\n",
        "      video.release()\n",
        "      print(f\"Découpage video {video_path} terminé. {frame_cpt} images ont été extraites.\")"
      ],
      "metadata": {
        "id": "symLKVeWsB59"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"asl.mp4\"  # Entrez le chemin de la vidéo ici"
      ],
      "metadata": {
        "id": "xYM_d6zOH3vm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_folder = \"output_images\"\n",
        "decoupeVideo(video_path, output_folder)"
      ],
      "metadata": {
        "id": "S1MZdBqNtwd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traitement des images extraites"
      ],
      "metadata": {
        "id": "JFr3KuY9ua6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les fonctions suivantes sont déjà présente dans le notebook précédent mais étant nécessaire à la prédiction nous les avons réécrites ici."
      ],
      "metadata": {
        "id": "XE70T6ROv0Bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "!!! OpenCV charge les images au format BGR (Bleu, Vert, Rouge)\n",
        "Matplotlib attend les images au format RGB (Rouge, Vert, Bleu) !!!\n",
        "\"\"\"\n",
        "\n",
        "# La fonction est déjà présente en haut du notebook\n",
        "def zoomMain(filename):\n",
        "    \"\"\"\n",
        "    Cette fonction permet de zoomer sur la main d'une image.\n",
        "    Ne renvoie pas d'erreur si une image n'est pas détectée.\n",
        "    De plus, le zoom sur la main génére une image qui écrase celle d'origine.\n",
        "\n",
        "    Arguments:\n",
        "    - filename: chemin vers l'image à zoomer\n",
        "    \"\"\"\n",
        "\n",
        "    mp_hands = mp.solutions.hands\n",
        "    hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5)\n",
        "    mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "    # Chargement image\n",
        "    image = cv2.imread(filename)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Conversion bgr --> rgb\n",
        "\n",
        "    # Détection de la main\n",
        "    results = hands.process(image_rgb)\n",
        "\n",
        "    if results.multi_hand_landmarks:\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "            # Récupérer les coordonnées des landmarks (points importants de la main)\n",
        "            h, w, _ = image.shape\n",
        "            landmarks = [(int(pt.x * w), int(pt.y * h)) for pt in hand_landmarks.landmark]\n",
        "\n",
        "            # Calculer la bounding box\n",
        "            x_min = min([pt[0] for pt in landmarks])\n",
        "            x_max = max([pt[0] for pt in landmarks])\n",
        "            y_min = min([pt[1] for pt in landmarks])\n",
        "            y_max = max([pt[1] for pt in landmarks])\n",
        "\n",
        "            # Ajout marge autour de la main\n",
        "            marge = 300  # On a décidé de mettre 300 car c'était ce qui nous paraissait le mieux après avoir fait divers test sur des images\n",
        "            x_min = max(0, x_min - marge)\n",
        "            y_min = max(0, y_min - marge)\n",
        "            x_max = min(w, x_max + marge)\n",
        "            y_max = min(h, y_max + marge)\n",
        "\n",
        "            # Extraction et zoom sur la main\n",
        "            hand_image = image[y_min:y_max, x_min:x_max]\n",
        "            hand_image_rgb = cv2.cvtColor(hand_image, cv2.COLOR_BGR2RGB)  # Conversion BGR -> RGB\n",
        "\n",
        "            # Enregistre la main zoomée\n",
        "            cv2.imwrite(filename, cv2.cvtColor(hand_image_rgb, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    # Fermeture Mediapipe\n",
        "    hands.close()"
      ],
      "metadata": {
        "id": "LiKOddGFuaCt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nous avons choisit d'implémenter cette fonction car celle fournit\n",
        "# par pytorch ne conservait les proportions d'images\n",
        "def resize_properly(image_path, output_path=\"*\", target_size=(300, 300)):\n",
        "    \"\"\"\n",
        "    Redimensionne une image proprement en gardant le bon ratio de l'image d'origine\n",
        "    Et a fait correspondre à une taille cible (target_size)\n",
        "    Ainsi les points important de l'image cible seront toujours là dans l'image redimensionnée\n",
        "\n",
        "    image_path: Chemin de l'image source\n",
        "    output_path: Chemin pour enregistrer l'image redimensionnée\n",
        "    target_size: Tuple (largeur, hauteur) de la taille cible\n",
        "    \"\"\"\n",
        "\n",
        "    # Si \"*\" alors on écrase l'image d'origine\n",
        "    if output_path == \"*\":\n",
        "      output_path= image_path\n",
        "\n",
        "    target_largeur, target_hauteur = target_size\n",
        "\n",
        "    # Ouverture l'image\n",
        "    image = Image.open(image_path)\n",
        "    image = ImageOps.exif_transpose(image)  # Garde l'orientation de l'image d'origine (verticale ou horizontale)\n",
        "    original_largeur, original_hauteur = image.size\n",
        "    ratio = original_largeur / original_hauteur\n",
        "\n",
        "    # Calcule nouvelle taille de l'image en conservant le ratio\n",
        "    if ratio > target_largeur / target_hauteur:\n",
        "        # Si image cible plus large que haute\n",
        "        # --> On ajuste la hauteur\n",
        "        new_largeur = target_largeur\n",
        "        new_hauteur = int(target_largeur / ratio)  # un nb de pixel est un entier\n",
        "    else:\n",
        "        # Si image plus haute que large\n",
        "        # --> On ajuste la largeur\n",
        "        new_hauteur = target_hauteur\n",
        "        new_largeur = int(target_hauteur * ratio)\n",
        "\n",
        "    # Redimensionne l'image avec le bon ratio\n",
        "    resized_image = image.resize((new_largeur, new_hauteur))\n",
        "\n",
        "    # Padding en noir de l'image pour avoir la taille cible\n",
        "    new_image = Image.new(\"RGB\", target_size, (0, 0, 0))  # (0, 0, 0) rgb --> noir\n",
        "    new_image.paste(resized_image,\n",
        "                    ((target_largeur - new_largeur) // 2, (target_hauteur - new_hauteur) // 2))\n",
        "                    # copie de l'image redimensionnée dans le cadre noir de bonne dimension\n",
        "                    # en la centrant au milieu du fond noir\n",
        "\n",
        "    # sauvergarde\n",
        "    new_image.save(output_path)"
      ],
      "metadata": {
        "id": "3CYPDCw1vr-4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir = 'output_images'\n",
        "\n",
        "for image in os.listdir(dir):\n",
        "    image_path = os.path.join(dir, image)\n",
        "\n",
        "    if os.path.isfile(image_path):\n",
        "        zoomMain(image_path)\n",
        "        resize_properly(image_path, \"*\", (200,200))\n",
        "print(\"Images modifiées\")"
      ],
      "metadata": {
        "id": "uA7_px2PNSXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prédiction sur vidéo"
      ],
      "metadata": {
        "id": "HG7GpTiQ46m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def word_prediction():\n",
        "  val_images = []\n",
        "\n",
        "  for dirname, _, filenames in os.walk('output_images'):\n",
        "      for filename in filenames:\n",
        "\n",
        "          path = os.path.join(dirname, filename)\n",
        "          img = Image.open(path)\n",
        "          img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "          pred_labels = []\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "              for _ in range(5):  # fait 5 prédictions en garde celle qui à la plus grande occurence\n",
        "                  output = model(img_tensor)  # Prédiction\n",
        "                  pred_labels.append(output.argmax(1).item())  # stockage des labels prédits 5 fois\n",
        "\n",
        "          # Compte les occurrences des prédictions\n",
        "          cpt = Counter(pred_labels)\n",
        "          val_most_frequent = cpt.most_common(1)[0][0]  # Classe prédite avec la + grande occurrence\n",
        "\n",
        "          val_images.append(val_most_frequent)\n",
        "\n",
        "  return val_images"
      ],
      "metadata": {
        "id": "Shn5SoaiMA1H"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le code suivant permet d'afficher la phrase ou le mot prédit par le modèle:"
      ],
      "metadata": {
        "id": "1NAWvx4GRS8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_images = word_prediction()\n",
        "\n",
        "next_pred = \"\"\n",
        "print(\"La phrase ou le mot de la vidéo est:\")\n",
        "for val in val_images:\n",
        "  pred = label_to_class[val]\n",
        "\n",
        "  if pred != next_pred:  # On empeche que le mot comporte 2 meme lettre à la suite\n",
        "    print(pred, end=\"\")\n",
        "    next_pred = pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iQ_v0-EO7su",
        "outputId": "97d2402a-b3bd-49c3-860c-76ab7ee037ee"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La phrase ou le mot de la vidéo est:\n",
            "BTITITXITIXBTITETUITITYITIBIVTUEFITBXVTITXTXTITOITBTXIXBTVTLQITBUIT"
          ]
        }
      ]
    }
  ]
}